Intro
=====


Words are a strange entity, I like to think of them as operators.

If we think of the collection of our mental states as a set, then we might as well begin the debate whether we can apply that awesome branch of Abstract Algebra: **Group Theory**.  

## Crash Course On Groups In words:

A group closure is when your set of words (operations) can lead you back to a situation (state) where you can repeat the prior movement.

## Cutting to the chase -- I gotta sleep too

Word groups are interesting -- group closures in language are a phenomenon occuring often in poetry, music lyrics, hypnosis, and rhetorical speeches. 

The set of words {"you","me"} is basically a group, occuring in an alternating fashion in the accented beat of musical choruses:

```lyrics
Happy Together by the Turtles 

I can't see me lovin' nobody but **you** 
For all my life 
When you're with me, baby the skies'll be blue 
For all my life 

Me and you and you and **me** 
No matter how they toss the dice, it has to be 
The only one for me is you, and you for me 
So happy **together** 

I can't see me lovin' nobody but **you** 
For all my life 
When you're with me, baby the skies'll be blue 
For all my life 

Me and you and you and **me** 
No matter how they toss the dice, it has to be 
The only one for me is you, and you for me 
So happy **together**
```

{you,me} is basically a group, but with enough {you,me} operations, the "together" becomes more probable Group closure in the **local** sense). 
Now bringing us to the question of how can we find a group in a systematic manner?

## The algorithm

We should expect (for smaller groups), a simple proportion of frequency. In so far as the group has a high %-of-closure, we would expect their usage to track eachother by a scaling factor.

The scaling factor exists to account for 
1) sometimes an operation will map back to other things which map back to it (% of time it leaves the group)
2) sometimes an operation will map back to itself.

Which brings up the point (which I should have spoken of earlier), that these words are being treated in this model as Random Functions (see Notes).






Notes: I'm not saying that there is intrinsic randomness -- each situation may indeed be deterministic -- *but* contextual sensitivity (words have this) *and the fact that* they have a limited number of probably end mappings, means that the group of possibilities is often shuffled well enough to assume so. 
From my experience, modeling the outcome as deterministic sets one up to wind-up going "all in" on a specific outcome, where there is high contextual sensitivity creating great complications if you fail, and demanding that a large amount of energy be withheld in the case of a disastrous mis-calculation/mis-step (much like a **bad-beat** in hold-em poker).
Best to play the odds, and rely on the Law-of-large numbers to reduce the fractional variance (making the outcome immeasureably more reliable).




https://books.google.com/ngrams/graph?content=hot%2C+cold&year_start=1800&year_end=2000&corpus=0&smoothing=3&share=&direct_url=t1%3B%2Chot%3B%2Cc0%3B.t1%3B%2Ccold%3B%2Cc0
